{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 개선\n",
    "- 말뭉치에 포함된 어휘 수가 많아지면 계산량이 커져 모델의 게산 시간이 너무 오래 걸린다는 단점이 존재함\n",
    "- Word2Vec 모델에 두 가지 개선사항을 반영하여 모델의 속도 개선을 이루고자 함\n",
    " 1. 'Embedding'이라는 새로운 게층 도입\n",
    " 2. '네거티브 샘플링'이라는 새로운 손실 함수 도입\n",
    "- 예를 들어, 어휘가 100만 개, 은닉층의 뉴런이 100개인 2계층 CBoW 모델일 경우\n",
    " * 입력층의 원핫 표현과 가중치 행렬 W_in의 곱 계산\n",
    "  > Embedding 게층 도입으로 문제 해결 가능\n",
    " * 은닉층과 가중치 행렬 W_out의 곱 및 softmax 계층의 계산\n",
    "  > 네거티브 샘플링 손실 함수 도입으로 문제 해결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 계층\n",
    "- 가중치 매개변수로부터 '단어 ID에 해당하는 행(벡터)'을 추출하는 계층으로, Embedding 계층에 단어 임베딩(분산 표현)을 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[6 7 8]\n",
      "[15 16 17]\n",
      "[[ 3  4  5]\n",
      " [ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 0  1  2]]\n"
     ]
    }
   ],
   "source": [
    "### 행렬에서 특정 행 추출하기\n",
    "import numpy as np\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "print(W)\n",
    "print(W[2])\n",
    "print(W[5])\n",
    "\n",
    "idx = np.array([1, 0, 3, 0])\n",
    "print(W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding 계층 구현\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    \n",
    "    ### Embedding 계층의 순전파는 가중치 W의 특정 행만 추출함\n",
    "    ### 단순히 가중치의 특정 행 뉴런만을 (아무것도 손대지 않고) 다음 층으로 흘려보낸 것\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    ### 역전파에서는 앞 층(출력 측 층)으로부터 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 됨\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "#         dW[self.idx] = dout   ### idx의 원소가 중복될 때 문제 발생\n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "#         np.add.at(dW, self.idx, dout)   ### 사용가능한 다른 방법 (효율이 더 좋음)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네거티브 샘플링\n",
    "- Softmax 대신 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 낮은 수준으로 일정하게 억제할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 은닉층 이후 계산의 문제점\n",
    " * 예를 들어, 어휘가 100만 개, 은닉층의 뉴런이 100개인 CBoW 모델에 Embedding 계층을 도입한 경우\n",
    "  > - 은닉층의 뉴런과 가중치 행렬 W_out의 곱 : 거대한 행렬의 곱을 계산하는 문제 <br>\n",
    "  > - Softmax 계층의 계산 : 어휘가 많아지면 softmax의 계산량도 증가함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다중 분류에서 이진 분류로 \n",
    " * 다중 분류 : 맥락이 주어졌을 때 정답이 되는 단어 하나를 선택하는 문제\n",
    "  > ex. 맥락이 'you'와 'goodbye'일 때, 타깃 단어는 무엇입니까?\n",
    " * 이진 분류 : 'Yes/No'로 답하는 문제\n",
    "  > ex. 맥락이 'you'와 'goodbye'일 때, 타깃 단어는 'say'입니까?\n",
    " * 은닉층과 출력 측의 가중치 행렬의 내적은 'say'에 해당하는 열(단어 벡터)만을 추출하고, 그 추출된 벡터와 은닉층 뉴런과의 내적을 계산하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)   ### 내적 계산\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시그모이드 함수와 교차 엔트로피 오차\n",
    " * 흔히 이진 분류 신경망에서는 점수에 시그모이드 함수를 적용해 확률로 변환하고, 손실을 구할 때는 교차 엔트로피 오차를 사용함\n",
    "  > 다중 분류 : 소프트맥스 함수 & 교차 엔트로피 오차 <br>\n",
    "  > 이진 분류 : 시그모이드 함수 & 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네거티브 샘플링 기법\n",
    " * 적은 수의 부정적 예를 샘플링해서 사용하는 것\n",
    "  > - 다중 분류 문제를 이진 분류로 다루려면 '정답(긍정적 예)'과 '오답(부정적 예)' 각각에 대해 바르게 (이진)분류할 수 있어야 함\n",
    "  > - 하지만 모든 부정적 예를 대상으로 하는 방법은 어휘 수가 늘어나면 감당할 수 없어 근사적인 해법으로 네거티브 샘플링을 사용함\n",
    " * 네거티브 샘플링은 각각의 데이터(긍정적 예와 샘플링된 부정적 예)의 손실을 더한 값을 최종 손실로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네거티브 샘플링과 샘플링 기법\n",
    " * 부정적 예를 샘플링하는 방법\n",
    "  > - 말뭉치에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어(희소 단어)를 적게 추출함\n",
    "  > - 각 단어의 출현 횟수에 대한 확률분포를 토대로 단어를 샘플링함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "you\n",
      "['I' '.' 'hello' '.' 'goodbye']\n",
      "['.' 'hello' 'goodbye' 'you' 'I']\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "### 0에서 9까지의 숫자 중 하나를 무작위로 샘플링\n",
    "print(np.random.choice(10))\n",
    "print(np.random.choice(10))\n",
    "\n",
    "### words에서 하나만 무작위로 샘플링\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "print(np.random.choice(words))\n",
    "\n",
    "### 5개만 무작위로 샘플링(중복 있음)\n",
    "print(np.random.choice(words, size=5))\n",
    "\n",
    "### 5개만 무작위로 샘플링(중복 없음)\n",
    "print(np.random.choice(words, size=5, replace=False))\n",
    "\n",
    "### 획률분포에 따라 샘플링\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "print(np.random.choice(words, p=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "### 네커티브 샘플링에서는 기본 확률분포에 0.75 제곱하는 것을 권고함\n",
    "### 이는 출현 확룰이 낮은 단어를 '버리지 않기'위함\n",
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3]\n",
      " [1 2]\n",
      " [2 1]]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            p = self.word_p.copy()\n",
    "            target_idx = target[i]\n",
    "            p[target_idx] = 0\n",
    "            p /= p.sum()\n",
    "            negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        return negative_sample\n",
    "    \n",
    "    \n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 네거티브 샘플링 구현\n",
    "from common.layers import Embedding, SigmoidWithLoss\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size+1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size+1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        ### 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        ### 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1+i].forward(score, negative_label)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CBoW 모델 구현\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        ### 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        ### 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "        \n",
    "        ### 모든 가중치와 기울기를 배열에 모음\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        ### 인스턴스 변수에 단어의 분산 표현을 저장\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 21 / 9295 | 시간 1[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 41 / 9295 | 시간 3[s] | 손실 4.15\n",
      "| 에폭 1 |  반복 61 / 9295 | 시간 5[s] | 손실 4.12\n",
      "| 에폭 1 |  반복 81 / 9295 | 시간 6[s] | 손실 4.05\n",
      "| 에폭 1 |  반복 101 / 9295 | 시간 8[s] | 손실 3.93\n",
      "| 에폭 1 |  반복 121 / 9295 | 시간 10[s] | 손실 3.79\n",
      "| 에폭 1 |  반복 141 / 9295 | 시간 12[s] | 손실 3.64\n",
      "| 에폭 1 |  반복 161 / 9295 | 시간 13[s] | 손실 3.49\n",
      "| 에폭 1 |  반복 181 / 9295 | 시간 15[s] | 손실 3.37\n",
      "| 에폭 1 |  반복 201 / 9295 | 시간 17[s] | 손실 3.27\n",
      "| 에폭 1 |  반복 221 / 9295 | 시간 18[s] | 손실 3.17\n",
      "| 에폭 1 |  반복 241 / 9295 | 시간 20[s] | 손실 3.10\n",
      "| 에폭 1 |  반복 261 / 9295 | 시간 22[s] | 손실 3.02\n",
      "| 에폭 1 |  반복 281 / 9295 | 시간 24[s] | 손실 2.95\n",
      "| 에폭 1 |  반복 301 / 9295 | 시간 25[s] | 손실 2.92\n",
      "| 에폭 1 |  반복 321 / 9295 | 시간 27[s] | 손실 2.86\n",
      "| 에폭 1 |  반복 341 / 9295 | 시간 29[s] | 손실 2.82\n",
      "| 에폭 1 |  반복 361 / 9295 | 시간 30[s] | 손실 2.81\n",
      "| 에폭 1 |  반복 381 / 9295 | 시간 32[s] | 손실 2.79\n",
      "| 에폭 1 |  반복 401 / 9295 | 시간 34[s] | 손실 2.74\n",
      "| 에폭 1 |  반복 421 / 9295 | 시간 36[s] | 손실 2.74\n",
      "| 에폭 1 |  반복 441 / 9295 | 시간 38[s] | 손실 2.73\n",
      "| 에폭 1 |  반복 461 / 9295 | 시간 39[s] | 손실 2.70\n",
      "| 에폭 1 |  반복 481 / 9295 | 시간 41[s] | 손실 2.69\n",
      "| 에폭 1 |  반복 501 / 9295 | 시간 43[s] | 손실 2.68\n",
      "| 에폭 1 |  반복 521 / 9295 | 시간 45[s] | 손실 2.66\n",
      "| 에폭 1 |  반복 541 / 9295 | 시간 47[s] | 손실 2.68\n",
      "| 에폭 1 |  반복 561 / 9295 | 시간 49[s] | 손실 2.66\n",
      "| 에폭 1 |  반복 581 / 9295 | 시간 50[s] | 손실 2.64\n",
      "| 에폭 1 |  반복 601 / 9295 | 시간 52[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 621 / 9295 | 시간 53[s] | 손실 2.63\n",
      "| 에폭 1 |  반복 641 / 9295 | 시간 55[s] | 손실 2.63\n",
      "| 에폭 1 |  반복 661 / 9295 | 시간 57[s] | 손실 2.59\n",
      "| 에폭 1 |  반복 681 / 9295 | 시간 58[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 701 / 9295 | 시간 60[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 721 / 9295 | 시간 61[s] | 손실 2.61\n",
      "| 에폭 1 |  반복 741 / 9295 | 시간 63[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 761 / 9295 | 시간 64[s] | 손실 2.59\n",
      "| 에폭 1 |  반복 781 / 9295 | 시간 66[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 801 / 9295 | 시간 68[s] | 손실 2.59\n",
      "| 에폭 1 |  반복 821 / 9295 | 시간 69[s] | 손실 2.60\n",
      "| 에폭 1 |  반복 841 / 9295 | 시간 71[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 861 / 9295 | 시간 72[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 881 / 9295 | 시간 74[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 901 / 9295 | 시간 76[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 921 / 9295 | 시간 77[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 941 / 9295 | 시간 79[s] | 손실 2.56\n",
      "| 에폭 1 |  반복 961 / 9295 | 시간 80[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 981 / 9295 | 시간 82[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1001 / 9295 | 시간 84[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1021 / 9295 | 시간 85[s] | 손실 2.58\n",
      "| 에폭 1 |  반복 1041 / 9295 | 시간 87[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1061 / 9295 | 시간 88[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1081 / 9295 | 시간 90[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1101 / 9295 | 시간 92[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1121 / 9295 | 시간 93[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1141 / 9295 | 시간 95[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1161 / 9295 | 시간 96[s] | 손실 2.57\n",
      "| 에폭 1 |  반복 1181 / 9295 | 시간 98[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1201 / 9295 | 시간 100[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1221 / 9295 | 시간 101[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1241 / 9295 | 시간 103[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1261 / 9295 | 시간 105[s] | 손실 2.54\n",
      "| 에폭 1 |  반복 1281 / 9295 | 시간 106[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1301 / 9295 | 시간 108[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1321 / 9295 | 시간 110[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1341 / 9295 | 시간 111[s] | 손실 2.55\n",
      "| 에폭 1 |  반복 1361 / 9295 | 시간 113[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1381 / 9295 | 시간 115[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1401 / 9295 | 시간 116[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1421 / 9295 | 시간 118[s] | 손실 2.53\n",
      "| 에폭 1 |  반복 1441 / 9295 | 시간 119[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1461 / 9295 | 시간 121[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1481 / 9295 | 시간 123[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1501 / 9295 | 시간 124[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1521 / 9295 | 시간 126[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1541 / 9295 | 시간 127[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1561 / 9295 | 시간 129[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1581 / 9295 | 시간 131[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1601 / 9295 | 시간 132[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 1621 / 9295 | 시간 134[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1641 / 9295 | 시간 135[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 1661 / 9295 | 시간 137[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1681 / 9295 | 시간 139[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 1701 / 9295 | 시간 140[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 1721 / 9295 | 시간 142[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1741 / 9295 | 시간 144[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1761 / 9295 | 시간 145[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 1781 / 9295 | 시간 147[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1801 / 9295 | 시간 148[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1821 / 9295 | 시간 150[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1841 / 9295 | 시간 151[s] | 손실 2.52\n",
      "| 에폭 1 |  반복 1861 / 9295 | 시간 153[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 1881 / 9295 | 시간 155[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1901 / 9295 | 시간 156[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1921 / 9295 | 시간 158[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 1941 / 9295 | 시간 159[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 1961 / 9295 | 시간 161[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 1981 / 9295 | 시간 162[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2001 / 9295 | 시간 164[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 2021 / 9295 | 시간 165[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2041 / 9295 | 시간 167[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2061 / 9295 | 시간 169[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2081 / 9295 | 시간 170[s] | 손실 2.51\n",
      "| 에폭 1 |  반복 2101 / 9295 | 시간 172[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2121 / 9295 | 시간 173[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2141 / 9295 | 시간 175[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2161 / 9295 | 시간 176[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2181 / 9295 | 시간 178[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2201 / 9295 | 시간 180[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2221 / 9295 | 시간 181[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2241 / 9295 | 시간 183[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2261 / 9295 | 시간 184[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2281 / 9295 | 시간 186[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2301 / 9295 | 시간 187[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2321 / 9295 | 시간 189[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2341 / 9295 | 시간 190[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2361 / 9295 | 시간 192[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2381 / 9295 | 시간 193[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2401 / 9295 | 시간 195[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2421 / 9295 | 시간 196[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2441 / 9295 | 시간 198[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2461 / 9295 | 시간 200[s] | 손실 2.49\n",
      "| 에폭 1 |  반복 2481 / 9295 | 시간 201[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2501 / 9295 | 시간 203[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2521 / 9295 | 시간 204[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2541 / 9295 | 시간 206[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2561 / 9295 | 시간 207[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2581 / 9295 | 시간 209[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2601 / 9295 | 시간 210[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 2621 / 9295 | 시간 212[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2641 / 9295 | 시간 213[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2661 / 9295 | 시간 215[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2681 / 9295 | 시간 216[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2701 / 9295 | 시간 218[s] | 손실 2.50\n",
      "| 에폭 1 |  반복 2721 / 9295 | 시간 219[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2741 / 9295 | 시간 221[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2761 / 9295 | 시간 222[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 2781 / 9295 | 시간 224[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 2801 / 9295 | 시간 225[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2821 / 9295 | 시간 227[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2841 / 9295 | 시간 229[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2861 / 9295 | 시간 230[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 2881 / 9295 | 시간 232[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2901 / 9295 | 시간 233[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2921 / 9295 | 시간 235[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 2941 / 9295 | 시간 236[s] | 손실 2.48\n",
      "| 에폭 1 |  반복 2961 / 9295 | 시간 238[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 2981 / 9295 | 시간 239[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 241[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3021 / 9295 | 시간 242[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3041 / 9295 | 시간 244[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3061 / 9295 | 시간 245[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3081 / 9295 | 시간 247[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3101 / 9295 | 시간 248[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3121 / 9295 | 시간 250[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3141 / 9295 | 시간 251[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3161 / 9295 | 시간 253[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3181 / 9295 | 시간 255[s] | 손실 2.47\n",
      "| 에폭 1 |  반복 3201 / 9295 | 시간 256[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3221 / 9295 | 시간 258[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3241 / 9295 | 시간 259[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3261 / 9295 | 시간 261[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3281 / 9295 | 시간 262[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3301 / 9295 | 시간 264[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3321 / 9295 | 시간 265[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3341 / 9295 | 시간 267[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3361 / 9295 | 시간 268[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3381 / 9295 | 시간 270[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3401 / 9295 | 시간 271[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3421 / 9295 | 시간 273[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3441 / 9295 | 시간 274[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3461 / 9295 | 시간 276[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3481 / 9295 | 시간 277[s] | 손실 2.44\n",
      "| 에폭 1 |  반복 3501 / 9295 | 시간 279[s] | 손실 2.46\n",
      "| 에폭 1 |  반복 3521 / 9295 | 시간 280[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3541 / 9295 | 시간 282[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3561 / 9295 | 시간 283[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3581 / 9295 | 시간 285[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3601 / 9295 | 시간 287[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3621 / 9295 | 시간 288[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3641 / 9295 | 시간 290[s] | 손실 2.45\n",
      "| 에폭 1 |  반복 3661 / 9295 | 시간 291[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3681 / 9295 | 시간 293[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3701 / 9295 | 시간 294[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3721 / 9295 | 시간 296[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3741 / 9295 | 시간 297[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3761 / 9295 | 시간 299[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3781 / 9295 | 시간 300[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 3801 / 9295 | 시간 302[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3821 / 9295 | 시간 303[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3841 / 9295 | 시간 305[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3861 / 9295 | 시간 306[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 3881 / 9295 | 시간 308[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 3901 / 9295 | 시간 309[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 3921 / 9295 | 시간 311[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3941 / 9295 | 시간 312[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 3961 / 9295 | 시간 314[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 3981 / 9295 | 시간 316[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4001 / 9295 | 시간 317[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4021 / 9295 | 시간 319[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4041 / 9295 | 시간 320[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4061 / 9295 | 시간 322[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4081 / 9295 | 시간 323[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4101 / 9295 | 시간 325[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4121 / 9295 | 시간 326[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4141 / 9295 | 시간 328[s] | 손실 2.42\n",
      "| 에폭 1 |  반복 4161 / 9295 | 시간 329[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4181 / 9295 | 시간 331[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4201 / 9295 | 시간 332[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4221 / 9295 | 시간 334[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4241 / 9295 | 시간 335[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4261 / 9295 | 시간 337[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4281 / 9295 | 시간 338[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4301 / 9295 | 시간 340[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4321 / 9295 | 시간 342[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 4341 / 9295 | 시간 343[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4361 / 9295 | 시간 345[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4381 / 9295 | 시간 346[s] | 손실 2.40\n",
      "| 에폭 1 |  반복 4401 / 9295 | 시간 348[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4421 / 9295 | 시간 349[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4441 / 9295 | 시간 351[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4461 / 9295 | 시간 352[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4481 / 9295 | 시간 354[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4501 / 9295 | 시간 355[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4521 / 9295 | 시간 357[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4541 / 9295 | 시간 358[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4561 / 9295 | 시간 360[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4581 / 9295 | 시간 361[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4601 / 9295 | 시간 363[s] | 손실 2.41\n",
      "| 에폭 1 |  반복 4621 / 9295 | 시간 364[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4641 / 9295 | 시간 366[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4661 / 9295 | 시간 367[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4681 / 9295 | 시간 369[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4701 / 9295 | 시간 371[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4721 / 9295 | 시간 372[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 4741 / 9295 | 시간 374[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4761 / 9295 | 시간 375[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4781 / 9295 | 시간 377[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4801 / 9295 | 시간 378[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 4821 / 9295 | 시간 380[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4841 / 9295 | 시간 381[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4861 / 9295 | 시간 383[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 4881 / 9295 | 시간 384[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4901 / 9295 | 시간 386[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 4921 / 9295 | 시간 387[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 4941 / 9295 | 시간 389[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 4961 / 9295 | 시간 390[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 4981 / 9295 | 시간 392[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5001 / 9295 | 시간 393[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5021 / 9295 | 시간 395[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5041 / 9295 | 시간 397[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5061 / 9295 | 시간 398[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5081 / 9295 | 시간 400[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5101 / 9295 | 시간 401[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5121 / 9295 | 시간 403[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5141 / 9295 | 시간 404[s] | 손실 2.39\n",
      "| 에폭 1 |  반복 5161 / 9295 | 시간 406[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5181 / 9295 | 시간 407[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5201 / 9295 | 시간 409[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 5221 / 9295 | 시간 410[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5241 / 9295 | 시간 412[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5261 / 9295 | 시간 413[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5281 / 9295 | 시간 415[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5301 / 9295 | 시간 416[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5321 / 9295 | 시간 418[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5341 / 9295 | 시간 419[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5361 / 9295 | 시간 421[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5381 / 9295 | 시간 422[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5401 / 9295 | 시간 424[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5421 / 9295 | 시간 425[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5441 / 9295 | 시간 427[s] | 손실 2.36\n",
      "| 에폭 1 |  반복 5461 / 9295 | 시간 429[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5481 / 9295 | 시간 430[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5501 / 9295 | 시간 432[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5521 / 9295 | 시간 433[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5541 / 9295 | 시간 435[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5561 / 9295 | 시간 436[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5581 / 9295 | 시간 438[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5601 / 9295 | 시간 439[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5621 / 9295 | 시간 441[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5641 / 9295 | 시간 442[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5661 / 9295 | 시간 444[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5681 / 9295 | 시간 445[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5701 / 9295 | 시간 447[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5721 / 9295 | 시간 448[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5741 / 9295 | 시간 450[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5761 / 9295 | 시간 451[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5781 / 9295 | 시간 453[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 5801 / 9295 | 시간 454[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5821 / 9295 | 시간 456[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 5841 / 9295 | 시간 457[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5861 / 9295 | 시간 459[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 5881 / 9295 | 시간 461[s] | 손실 2.37\n",
      "| 에폭 1 |  반복 5901 / 9295 | 시간 462[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5921 / 9295 | 시간 464[s] | 손실 2.35\n",
      "| 에폭 1 |  반복 5941 / 9295 | 시간 465[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 5961 / 9295 | 시간 467[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 5981 / 9295 | 시간 468[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 470[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6021 / 9295 | 시간 471[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6041 / 9295 | 시간 473[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6061 / 9295 | 시간 474[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6081 / 9295 | 시간 476[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6101 / 9295 | 시간 478[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6121 / 9295 | 시간 479[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6141 / 9295 | 시간 482[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 6161 / 9295 | 시간 483[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6181 / 9295 | 시간 485[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 6201 / 9295 | 시간 487[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6221 / 9295 | 시간 489[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6241 / 9295 | 시간 490[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6261 / 9295 | 시간 492[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6281 / 9295 | 시간 494[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6301 / 9295 | 시간 496[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6321 / 9295 | 시간 497[s] | 손실 2.34\n",
      "| 에폭 1 |  반복 6341 / 9295 | 시간 499[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6361 / 9295 | 시간 501[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6381 / 9295 | 시간 502[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 6401 / 9295 | 시간 504[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6421 / 9295 | 시간 505[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6441 / 9295 | 시간 507[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6461 / 9295 | 시간 509[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6481 / 9295 | 시간 510[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6501 / 9295 | 시간 512[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6521 / 9295 | 시간 514[s] | 손실 2.33\n",
      "| 에폭 1 |  반복 6541 / 9295 | 시간 515[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6561 / 9295 | 시간 517[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6581 / 9295 | 시간 519[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6601 / 9295 | 시간 520[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6621 / 9295 | 시간 522[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 6641 / 9295 | 시간 524[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6661 / 9295 | 시간 525[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6681 / 9295 | 시간 527[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6701 / 9295 | 시간 528[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 6721 / 9295 | 시간 530[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6741 / 9295 | 시간 531[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6761 / 9295 | 시간 533[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6781 / 9295 | 시간 534[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6801 / 9295 | 시간 536[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6821 / 9295 | 시간 537[s] | 손실 2.32\n",
      "| 에폭 1 |  반복 6841 / 9295 | 시간 539[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 6861 / 9295 | 시간 540[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 6881 / 9295 | 시간 542[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6901 / 9295 | 시간 543[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 6921 / 9295 | 시간 545[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 6941 / 9295 | 시간 547[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 6961 / 9295 | 시간 548[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 6981 / 9295 | 시간 550[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7001 / 9295 | 시간 551[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7021 / 9295 | 시간 553[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7041 / 9295 | 시간 554[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7061 / 9295 | 시간 556[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7081 / 9295 | 시간 557[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7101 / 9295 | 시간 559[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7121 / 9295 | 시간 561[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7141 / 9295 | 시간 563[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7161 / 9295 | 시간 565[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7181 / 9295 | 시간 566[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7201 / 9295 | 시간 569[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7221 / 9295 | 시간 571[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7241 / 9295 | 시간 573[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7261 / 9295 | 시간 575[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7281 / 9295 | 시간 577[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7301 / 9295 | 시간 579[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7321 / 9295 | 시간 580[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7341 / 9295 | 시간 582[s] | 손실 2.31\n",
      "| 에폭 1 |  반복 7361 / 9295 | 시간 584[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7381 / 9295 | 시간 586[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7401 / 9295 | 시간 587[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7421 / 9295 | 시간 589[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7441 / 9295 | 시간 591[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7461 / 9295 | 시간 593[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7481 / 9295 | 시간 594[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7501 / 9295 | 시간 596[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7521 / 9295 | 시간 598[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7541 / 9295 | 시간 599[s] | 손실 2.30\n",
      "| 에폭 1 |  반복 7561 / 9295 | 시간 601[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7581 / 9295 | 시간 602[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7601 / 9295 | 시간 604[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7621 / 9295 | 시간 605[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7641 / 9295 | 시간 607[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7661 / 9295 | 시간 608[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7681 / 9295 | 시간 610[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7701 / 9295 | 시간 611[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7721 / 9295 | 시간 613[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7741 / 9295 | 시간 614[s] | 손실 2.29\n",
      "| 에폭 1 |  반복 7761 / 9295 | 시간 616[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7781 / 9295 | 시간 617[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7801 / 9295 | 시간 619[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 7821 / 9295 | 시간 620[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7841 / 9295 | 시간 622[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7861 / 9295 | 시간 624[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7881 / 9295 | 시간 625[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 7901 / 9295 | 시간 627[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7921 / 9295 | 시간 628[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 7941 / 9295 | 시간 630[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 7961 / 9295 | 시간 631[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 7981 / 9295 | 시간 633[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8001 / 9295 | 시간 634[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8021 / 9295 | 시간 636[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8041 / 9295 | 시간 637[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8061 / 9295 | 시간 639[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8081 / 9295 | 시간 640[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8101 / 9295 | 시간 642[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8121 / 9295 | 시간 643[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8141 / 9295 | 시간 645[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8161 / 9295 | 시간 646[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8181 / 9295 | 시간 648[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8201 / 9295 | 시간 649[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8221 / 9295 | 시간 651[s] | 손실 2.28\n",
      "| 에폭 1 |  반복 8241 / 9295 | 시간 653[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8261 / 9295 | 시간 654[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8281 / 9295 | 시간 656[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8301 / 9295 | 시간 657[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8321 / 9295 | 시간 659[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8341 / 9295 | 시간 660[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8361 / 9295 | 시간 662[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8381 / 9295 | 시간 663[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8401 / 9295 | 시간 665[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8421 / 9295 | 시간 666[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8441 / 9295 | 시간 668[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8461 / 9295 | 시간 669[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8481 / 9295 | 시간 671[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8501 / 9295 | 시간 672[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8521 / 9295 | 시간 674[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8541 / 9295 | 시간 675[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8561 / 9295 | 시간 677[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8581 / 9295 | 시간 678[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8601 / 9295 | 시간 680[s] | 손실 2.19\n",
      "| 에폭 1 |  반복 8621 / 9295 | 시간 682[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8641 / 9295 | 시간 683[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8661 / 9295 | 시간 685[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8681 / 9295 | 시간 686[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8701 / 9295 | 시간 688[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8721 / 9295 | 시간 689[s] | 손실 2.26\n",
      "| 에폭 1 |  반복 8741 / 9295 | 시간 691[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8761 / 9295 | 시간 692[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 8781 / 9295 | 시간 694[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8801 / 9295 | 시간 695[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8821 / 9295 | 시간 697[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 8841 / 9295 | 시간 698[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8861 / 9295 | 시간 700[s] | 손실 2.20\n",
      "| 에폭 1 |  반복 8881 / 9295 | 시간 702[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8901 / 9295 | 시간 703[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8921 / 9295 | 시간 705[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 8941 / 9295 | 시간 706[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 8961 / 9295 | 시간 708[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 8981 / 9295 | 시간 709[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 9001 / 9295 | 시간 711[s] | 손실 2.25\n",
      "| 에폭 1 |  반복 9021 / 9295 | 시간 712[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9041 / 9295 | 시간 714[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9061 / 9295 | 시간 715[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9081 / 9295 | 시간 717[s] | 손실 2.19\n",
      "| 에폭 1 |  반복 9101 / 9295 | 시간 718[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 9121 / 9295 | 시간 720[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9141 / 9295 | 시간 721[s] | 손실 2.27\n",
      "| 에폭 1 |  반복 9161 / 9295 | 시간 723[s] | 손실 2.21\n",
      "| 에폭 1 |  반복 9181 / 9295 | 시간 724[s] | 손실 2.18\n",
      "| 에폭 1 |  반복 9201 / 9295 | 시간 726[s] | 손실 2.24\n",
      "| 에폭 1 |  반복 9221 / 9295 | 시간 727[s] | 손실 2.23\n",
      "| 에폭 1 |  반복 9241 / 9295 | 시간 729[s] | 손실 2.22\n",
      "| 에폭 1 |  반복 9261 / 9295 | 시간 731[s] | 손실 2.19\n",
      "| 에폭 1 |  반복 9281 / 9295 | 시간 732[s] | 손실 2.21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CBoW 모델 학습\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import create_contexts_target, to_cpu\n",
    "from dataset import ptb\n",
    "\n",
    "### 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 1\n",
    "\n",
    "### 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "### 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "### 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "### 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = './output/cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.98193359375\n",
      " i: 0.9775390625\n",
      " why: 0.97705078125\n",
      " 'm: 0.95703125\n",
      " 're: 0.955078125\n",
      "\n",
      "[query] year\n",
      " month: 0.91943359375\n",
      " week: 0.88330078125\n",
      " earlier: 0.8798828125\n",
      " robust: 0.83984375\n",
      " quarter: 0.83447265625\n",
      "\n",
      "[query] car\n",
      " book: 0.9697265625\n",
      " dialing: 0.966796875\n",
      " strike: 0.966796875\n",
      " corporation: 0.96142578125\n",
      " virus: 0.9609375\n",
      "\n",
      "[query] toyota\n",
      " royal: 0.9453125\n",
      " steel: 0.94287109375\n",
      " fuel: 0.939453125\n",
      " mobile: 0.93798828125\n",
      " wcrs: 0.93798828125\n"
     ]
    }
   ],
   "source": [
    "from common.util import most_similar\n",
    "\n",
    "pkl_file = './output/cbow_params.pkl'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    \n",
    "word_vecs = params['word_vecs']\n",
    "word_to_id = params['word_to_id']\n",
    "id_to_word = params['id_to_word']\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " share: 5.1328125\n",
      " spokesman: 4.69140625\n",
      " know: 4.390625\n",
      " problem: 4.33984375\n",
      " lot: 4.31640625\n",
      "None\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " share: 4.87109375\n",
      " cents: 4.46484375\n",
      " yield: 4.39453125\n",
      " yen: 3.904296875\n",
      " composite: 3.568359375\n",
      "None\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " yield: 4.06640625\n",
      " cents: 3.94140625\n",
      " million: 3.931640625\n",
      " revenue: 3.751953125\n",
      " president: 3.66015625\n",
      "None\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " do: 4.46875\n",
      " n't: 4.390625\n",
      " know: 4.1953125\n",
      " composite: 4.1015625\n",
      " you: 4.03125\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from common.util import analogy\n",
    "print(analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs))\n",
    "print(analogy('take', 'took', 'go', word_to_id, id_to_word, word_vecs))\n",
    "print(analogy('car', 'cars', 'child', word_to_id, id_to_word, word_vecs))\n",
    "print(analogy('good', 'better', 'bad', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유추 문제에 의한 단어 벡터의 평가 결과\n",
    " * 모델에 따라 정확도가 다름 -> 말뭉치에 따라 적합한 모델 선택\n",
    " * 일반적으로 말뭉치가 클수록 결과가 좋음 -> 항상 데이터가 많은 게 좋음\n",
    " * 단어 벡터 차원 수는 적당한 크기가 좋음 -> 너무 커도 정확도가 나빠짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "- Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출함\n",
    "- Word2Vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋음\n",
    "- 네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있음\n",
    "- Word2Vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치함\n",
    "- Word2Vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 뎃셈과 뺄셈으로 풀 수 있음\n",
    "- Word2Vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
